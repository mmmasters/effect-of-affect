{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f46fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re \n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from emotion import*\n",
    "import emoji\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8732c4",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "Using Twitter's API to search for the FakeNewsNet tweet IDs resulted in four CSV files. \n",
    "- Filter out non-english language tweets and any tweets that are replies, retweets or quotes\n",
    "- Combine four files into one dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cda6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant data\n",
    "col_list = [0,2,3,4,10,11,16, 24, 25, 26, 50, 51, 53]\n",
    "politifact_fake_df = pd.read_csv('politifact_fake_flat.csv', \n",
    "                                 usecols=col_list, index_col=0)\n",
    "politifact_real_df = pd.read_csv('politifact_real_flat.csv', \n",
    "                                 usecols=col_list, index_col=0)\n",
    "gossipcop_fake_df = pd.read_csv('gossipcop_fake_flat.csv', \n",
    "                                usecols=col_list, index_col=0)\n",
    "gossipcop_real_df = pd.read_csv('gossipcop_real_flat.csv', \n",
    "                                usecols=col_list, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbcfb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "df_names = politifact_fake_df.rename({'referenced_tweets.replied_to.id': 'reply', \n",
    "                                      'referenced_tweets.retweeted.id': 'retweet', \n",
    "                                      'referenced_tweets.quoted.id': 'quote', \n",
    "                                      'entities.hashtags': 'hashtags', \n",
    "                                      'author.public_metrics.followers_count': 'followers', \n",
    "                                      'author.public_metrics.following_count': 'following', \n",
    "                                      'author.public_metrics.tweet_count': 'tweet_count', \n",
    "                                      'public_metrics.retweet_count': 'num_retweets', \n",
    "                                      'entities.mentions': 'mentions', \n",
    "                                      'entities.urls':'urls'}, axis=1)\n",
    "pr_names = politifact_real_df.rename({'referenced_tweets.replied_to.id': 'reply', \n",
    "                                      'referenced_tweets.retweeted.id': 'retweet', \n",
    "                                      'referenced_tweets.quoted.id': 'quote', \n",
    "                                      'entities.hashtags': 'hashtags', \n",
    "                                      'author.public_metrics.followers_count': 'followers', \n",
    "                                      'author.public_metrics.following_count': 'following', \n",
    "                                      'author.public_metrics.tweet_count': 'tweet_count', \n",
    "                                      'public_metrics.retweet_count': 'num_retweets', \n",
    "                                      'entities.mentions': 'mentions', \n",
    "                                      'entities.urls':'urls'}, axis=1)\n",
    "gf_names = gossipcop_fake_df.rename({'referenced_tweets.replied_to.id': 'reply', \n",
    "                                      'referenced_tweets.retweeted.id': 'retweet', \n",
    "                                      'referenced_tweets.quoted.id': 'quote', \n",
    "                                      'entities.hashtags': 'hashtags', \n",
    "                                      'author.public_metrics.followers_count': 'followers', \n",
    "                                      'author.public_metrics.following_count': 'following', \n",
    "                                      'author.public_metrics.tweet_count': 'tweet_count', \n",
    "                                      'public_metrics.retweet_count': 'num_retweets', \n",
    "                                      'entities.mentions': 'mentions', \n",
    "                                      'entities.urls':'urls'}, axis=1)\n",
    "gr_names = gossipcop_real_df.rename({'referenced_tweets.replied_to.id': 'reply', \n",
    "                                      'referenced_tweets.retweeted.id': 'retweet', \n",
    "                                      'referenced_tweets.quoted.id': 'quote', \n",
    "                                      'entities.hashtags': 'hashtags', \n",
    "                                      'author.public_metrics.followers_count': 'followers', \n",
    "                                      'author.public_metrics.following_count': 'following', \n",
    "                                      'author.public_metrics.tweet_count': 'tweet_count', \n",
    "                                      'public_metrics.retweet_count': 'num_retweets', \n",
    "                                      'entities.mentions': 'mentions', \n",
    "                                      'entities.urls':'urls'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7da402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter out non-english tweets and any that aren't original tweets\n",
    "def filter(df):\n",
    "    new_df=df[df.lang == \"en\"]\n",
    "    df1 = new_df[new_df['reply'].isnull()]\n",
    "    df2 = df1[df1['retweet'].isnull()]\n",
    "    df3 = df2[df2['quote'].isnull()]\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter\n",
    "filtered_politifact_fake_df = filter(df_names)\n",
    "filtered_politifact_real_df = filter(pr_names)\n",
    "filtered_gossipcop_fake_df = filter(gf_names)\n",
    "filtered_gossipcop_real_df = filter(gr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add labels indicating source of ground truth and veracity\n",
    "filtered_politifact_fake_df['source']='Politifact'\n",
    "filtered_politifact_fake_df['veracity']='Fake'\n",
    "filtered_politifact_real_df['source']='Politifact'\n",
    "filtered_politifact_real_df['veracity']='Real'\n",
    "filtered_gossipcop_fake_df['source']='Gossipcop'\n",
    "filtered_gossipcop_fake_df['veracity']='Fake'\n",
    "filtered_gossipcop_real_df['source']='Gossipcop'\n",
    "filtered_gossipcop_real_df['veracity']='Real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep important columns\n",
    "df1 = filtered_politifact_fake_df[['text', 'num_retweets', 'source', 'veracity', \n",
    "                                   'hashtags', 'followers', 'following', 'tweet_count', \n",
    "                                   'mentions', 'urls']].copy()\n",
    "df2 = filtered_politifact_real_df[['text', 'num_retweets', 'source', 'veracity', \n",
    "                                   'hashtags', 'followers', 'following', 'tweet_count', \n",
    "                                   'mentions', 'urls']].copy()\n",
    "df3 = filtered_gossipcop_fake_df[['text', 'num_retweets', 'source', 'veracity', \n",
    "                                  'hashtags', 'followers', 'following', 'tweet_count', \n",
    "                                  'mentions', 'urls']].copy()\n",
    "df4 = filtered_gossipcop_real_df[['text', 'num_retweets', 'source', 'veracity', \n",
    "                                  'hashtags', 'followers', 'following', 'tweet_count', \n",
    "                                  'mentions', 'urls']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a144f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all dataframes into one\n",
    "data = [df1, df2, df3, df4]\n",
    "fakeNewsNet = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2396b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save complete dataframe to csv\n",
    "fakeNewsNet.to_csv('fakeNewsNet.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5715b56",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Check for missingness\n",
    "- Count and remove hashtags, mentions and URLs\n",
    "- Remove emojis, repeated letters, numbers and stopwords from text\n",
    "- Make text lowercase and lemmatize\n",
    "- Investigate most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls - hashtags, mentions and urls will be calculated next\n",
    "fakeNewsNet.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate dataframe\n",
    "fakeNewsNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a37427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create hashtag count column and remove hashtag column\n",
    "hashtag_count = []\n",
    "for i in fakeNewsNet['hashtags']:\n",
    "    if pd.isna(i):\n",
    "        hashtag_count.append(0)\n",
    "    else:\n",
    "        hashtag_count.append(i.count('tag'))\n",
    "\n",
    "fakeNewsNet['num_hashtags'] = hashtag_count\n",
    "fakeNewsNet.drop('hashtags', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create mentions count column and remove mention column\n",
    "mentions_count = []\n",
    "for i in fakeNewsNet['mentions']:\n",
    "    if pd.isna(i):\n",
    "        mentions_count.append(0)\n",
    "    else:\n",
    "        mentions_count.append(i.count('username'))\n",
    "\n",
    "fakeNewsNet['num_mentions'] = mentions_count\n",
    "fakeNewsNet.drop('mentions', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a95f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create URL count column and remove URL column\n",
    "urls_count = []\n",
    "for i in fakeNewsNet['urls']:\n",
    "    if pd.isna(i):\n",
    "        urls_count.append(0)\n",
    "    else:\n",
    "        urls_count.append(i.count('\"url\"'))\n",
    "\n",
    "fakeNewsNet['num_urls'] = urls_count\n",
    "fakeNewsNet.drop('urls', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2829b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls again - none found\n",
    "fakeNewsNet.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save english stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "#Add extra stopwords as desired\n",
    "extra_stops = ['via', 'news', 'u', 'e', 'trump', 'amp', 'bieber', 'us']\n",
    "for i in extra_stops:\n",
    "    stopwords_english.append(i)\n",
    "\n",
    "#Initiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that removes usernames, URLS and \\n characters and shortens repeated letters\n",
    "def abbrev_text(text_list):\n",
    "    abbrev_text = []\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "\n",
    "        #Twitter handles removed to USERNAME \n",
    "        text = re.sub(r\"@[\\S]+\", \" \", text_list[i])\n",
    "\n",
    "        #Links changed to URL \n",
    "        text = re.sub(r\"(http://[\\S]+)|(https://[\\S]+)|(www\\.[\\S]+)\", \" \", text)\n",
    "\n",
    "        #Repeated letters curtailed to two\n",
    "        text = re.sub(r'(\\w)\\1(\\1+)', r'\\1\\1', text)\n",
    "        \n",
    "        #Remove \\n characters\n",
    "        final = text.replace('\\\\n', '')\n",
    "        \n",
    "        abbrev_text.append(final)\n",
    "    \n",
    "    return abbrev_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43641dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove emojis \n",
    "def remove_emojis(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5639180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function removes punctuation, numbers, capital letters and stopwords and lemmatizes \n",
    "def preprocess(text):\n",
    "    preprocessed_tokens = []\n",
    "    #Remove punctuation\n",
    "    stripped_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    for word in word_tokenize(stripped_text):\n",
    "        #Remove numbers, stopwords and make lowercase\n",
    "        word = word.lower()\n",
    "        if word.isalpha() and word not in stopwords_english:\n",
    "            preprocessed_tokens.append(lemmatizer.lemmatize(word))\n",
    "    preprocessed_words = \" \".join(preprocessed_tokens)\n",
    "    return preprocessed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ef134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean all text data in dataset\n",
    "abbrev_text_all = abbrev_text(fakeNewsNet['text'])\n",
    "clean_text = []\n",
    "\n",
    "for i in tqdm(range(len(abbrev_text_all))):\n",
    "    emoji_free = remove_emojis(abbrev_text_all[i])\n",
    "    preprocessed = preprocess(emoji_free)\n",
    "    if len(preprocessed)>0:\n",
    "        clean_text.append(preprocessed)\n",
    "    else: \n",
    "        clean_text.append(\"EMPTY\")\n",
    "        \n",
    "fakeNewsNet['clean_text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most popular words in the train data\n",
    "words = []\n",
    "\n",
    "#Make a list of all of the words in all of the tweets\n",
    "for i in fakeNewsNet['clean_text']:\n",
    "    for word in word_tokenize(i):\n",
    "        words.append(word)\n",
    "\n",
    "#Counter instance\n",
    "c_tweet = Counter(words)\n",
    "\n",
    "#Find 20 most popular words\n",
    "most_popular = c_tweet.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 20 most popular words\n",
    "sns.barplot(x=[count for word, count in most_popular], \n",
    "            y=[word for word, count in most_popular])\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c8347",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- Import VAD lexicon and build function to get VAD scores (NRC-EIL scores found with emotion-nrc-affect-lex package)\n",
    "- Filter out tweets with less than 3 hits on VAD lexicon\n",
    "- Find length of original tweet in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import lexicon\n",
    "nrc_vad_df = pd.read_csv('NRC-VAD-Lexicon.txt', \n",
    "                         names=[\"term\", \"valence\", \"arousal\", \"dominance\"], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform dataframe to dictionary for improved search time\n",
    "nrc_vad_dict = nrc_vad_df.set_index('term').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get vad and emotion scores from text\n",
    "def emotion(tweet):\n",
    "    token_tweet = word_tokenize(tweet)\n",
    "    #Initialize VAD to zero\n",
    "    valence = 0\n",
    "    arousal = 0\n",
    "    dominance = 0\n",
    "    vad_count = 0\n",
    "    \n",
    "    #Search for each word in VAD dictionary and sum values \n",
    "    for word in token_tweet:\n",
    "        vad_list = nrc_vad_dict.get(word, [0, 0, 0])\n",
    "        if vad_list[2] > 0:\n",
    "            vad_count += 1\n",
    "            valence += vad_list[0]\n",
    "            arousal += vad_list[1]\n",
    "            dominance += vad_list[2]\n",
    "    \n",
    "    #Account for zero division error and average VAD scores\n",
    "    if vad_count > 0: \n",
    "        val_tweet = valence/vad_count\n",
    "        aro_tweet = arousal/vad_count\n",
    "        dom_tweet = dominance/vad_count\n",
    "    else:\n",
    "        val_tweet = 0\n",
    "        aro_tweet = 0\n",
    "        dom_tweet = 0\n",
    "        \n",
    "    \n",
    "    #use emo to get emotion intensities then divide by count for tweet average\n",
    "    emo_object = Emo(word_tokenize(tweet))\n",
    "    weight = emo_object.weighted_emotion_scores\n",
    "    count = emo_object.raw_emotion_scores\n",
    "    try:\n",
    "        anger_tweet = weight.get(\"anger\", 0)/count.get(\"anger\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        anger_tweet = 0\n",
    "        \n",
    "    try:\n",
    "        anticipation_tweet = weight.get(\"anticipation\", 0)/count.get(\"anticipation\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        anticipation_tweet = 0\n",
    "        \n",
    "    try:\n",
    "        disgust_tweet = weight.get(\"disgust\", 0)/count.get(\"disgust\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        disgust_tweet = 0\n",
    "        \n",
    "    try:\n",
    "        fear_tweet = weight.get(\"fear\", 0)/count.get(\"fear\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        fear_tweet = 0\n",
    "        \n",
    "    try:\n",
    "        joy_tweet = weight.get(\"joy\", 0)/count.get(\"joy\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        joy_tweet = 0\n",
    "        \n",
    "    try:\n",
    "        sadness_tweet = weight.get(\"sadness\", 0)/count.get(\"sadness\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        sadness_tweet = 0\n",
    "        \n",
    "    try:\n",
    "        surprise_tweet = weight.get(\"surprise\", 0)/count.get(\"surprise\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        surprise_tweet = 0\n",
    "        \n",
    "    try:\n",
    "        trust_tweet = weight.get(\"trust\", 0)/count.get(\"trust\", 0)\n",
    "    except ZeroDivisionError:\n",
    "        trust_tweet = 0\n",
    "    \n",
    "    return val_tweet, aro_tweet, dom_tweet, anger_tweet, anticipation_tweet, disgust_tweet, fear_tweet, joy_tweet, sadness_tweet, surprise_tweet, trust_tweet, vad_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run through each tweet finding VAD and EIL scores\n",
    "val_list = []\n",
    "aro_list = []\n",
    "dom_list = []\n",
    "anger_list = []\n",
    "anticipation_list = []\n",
    "disgust_list = []\n",
    "fear_list = []\n",
    "joy_list = []\n",
    "sadness_list = []\n",
    "surprise_list = []\n",
    "trust_list = []\n",
    "vad_count_list = []\n",
    "\n",
    "for tweet in tqdm(fakeNewsNet['clean_text']):\n",
    "    val_list.append(emotion(tweet)[0])\n",
    "    aro_list.append(emotion(tweet)[1])\n",
    "    dom_list.append(emotion(tweet)[2])\n",
    "    anger_list.append(emotion(tweet)[3])\n",
    "    anticipation_list.append(emotion(tweet)[4])\n",
    "    disgust_list.append(emotion(tweet)[5])\n",
    "    fear_list.append(emotion(tweet)[6])\n",
    "    joy_list.append(emotion(tweet)[7])\n",
    "    sadness_list.append(emotion(tweet)[8])\n",
    "    surprise_list.append(emotion(tweet)[9])\n",
    "    trust_list.append(emotion(tweet)[10])\n",
    "    vad_count_list.append(emotion(tweet)[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add emotion metric scores to dataframe\n",
    "fakeNewsNet['valence'] = val_list\n",
    "fakeNewsNet['arousal'] = aro_list\n",
    "fakeNewsNet['dominance'] = dom_list\n",
    "fakeNewsNet['anger'] = anger_list\n",
    "fakeNewsNet['anticipation'] = anticipation_list\n",
    "fakeNewsNet['disgust'] = disgust_list\n",
    "fakeNewsNet['fear'] = fear_list\n",
    "fakeNewsNet['joy'] = joy_list\n",
    "fakeNewsNet['sadness'] = sadness_list\n",
    "fakeNewsNet['surprise'] = surprise_list\n",
    "fakeNewsNet['trust'] = trust_list\n",
    "fakeNewsNet['vad_count'] = vad_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e010ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Investigate dataframe\n",
    "fakeNewsNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38006bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv\n",
    "fakeNewsNet.to_csv('fakeNewsNet_emotionscores.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb1de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count totals\n",
    "df_count = fakeNewsNet.groupby(['veracity']).count()\n",
    "df_count[df_count.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to only those instances with 3 or more VAD scores\n",
    "fNN_compact = fakeNewsNet[fakeNewsNet.vad_count > 2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add length of original text as feature\n",
    "fNN_compact['tweet_length'] = fNN_compact['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save new smaller dataset\n",
    "fNN_compact.to_csv('fNN_compact.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00738afa",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['followers', 'following', 'tweet_count', 'num_hashtags', \n",
    "                'num_mentions', 'num_urls', 'tweet_length', 'valence', \n",
    "                'arousal', 'dominance', 'anger', 'anticipation', 'disgust', \n",
    "                'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "#split dataset in features and target variable\n",
    "X = fNN_compact[feature_cols]\n",
    "y = fNN_compact['veracity']\n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=42)\n",
    "\n",
    "#Split test into test and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                    stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43417d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save separate datasets\n",
    "X_train.to_csv('X_train.csv', index = False, header=True)\n",
    "X_val.to_csv('X_val.csv', index = False, header=True)\n",
    "X_test.to_csv('X_test.csv', index = False, header=True)\n",
    "y_train.to_csv('y_train.csv', index = False, header=True)\n",
    "y_val.to_csv('y_val.csv', index = False, header=True)\n",
    "y_test.to_csv('y_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count totals - X_train\n",
    "df_train = X_train.join(y_train)\n",
    "train_count = df_train.groupby(['veracity']).count()\n",
    "train_count[train_count.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count totals - X_val\n",
    "df_val = X_val.join(y_val)\n",
    "val_count = df_val.groupby(['veracity']).count()\n",
    "val_count[val_count.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count totals - X_test\n",
    "df_test = X_test.join(y_test)\n",
    "test_count = df_test.groupby(['veracity']).count()\n",
    "df_test[test_count.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f52c9",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "- Check correlations\n",
    "- Univariate analysis visualisations for non-emotion metrics\n",
    "- Welch's t-tests for emotion metrics\n",
    "- Effect size for emotion metrics\n",
    "- Visualisations for emotion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453763d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join train X and y into one train dataframe\n",
    "fNN_train = X_train.join(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08196cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlations\n",
    "fNN_corr = fNN_train.drop(['veracity'], axis=1)\n",
    "\n",
    "sns.heatmap(fNN_corr.corr(), cmap=\"vlag\")\n",
    "plt.show\n",
    "fNN_corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f35e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive statistics\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "print(fNN_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec03182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise lengths\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.countplot(x = fNN_train['tweet_length'], palette=\"Blues_d\")\n",
    "plt.title('Tweet Length Distribution', fontsize = 12)\n",
    "plt.xlabel('Words per Tweet', fontsize = 12)\n",
    "plt.ylabel('Number of Tweets', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93806bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare tweet lengths \n",
    "sns.violinplot(y=fNN_train[\"tweet_length\"], x=fNN_train[\"veracity\"])\n",
    "plt.title('Tweet Length by Veracity', fontsize = 12)\n",
    "plt.ylabel('Words per Tweet', fontsize = 12)\n",
    "plt.xlabel('Veracity', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f345d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin plots for non-emotion metrics - log(n+1) transformed for clearer visualisation\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(12, 12))\n",
    "\n",
    "#Compare followers\n",
    "sns.violinplot(ax=ax1, y=np.log10(fNN_train[\"followers\"]+1), x=fNN_train[\"veracity\"])\n",
    "ax1.set_title('Number of followers by Veracity')\n",
    "ax1.set_ylabel('log(followers+1)')\n",
    "\n",
    "#Compare following\n",
    "sns.violinplot(ax=ax2, y=np.log10(fNN_train[\"following\"]+1), x=fNN_train[\"veracity\"])\n",
    "ax2.set_title('Number following by Veracity')\n",
    "ax2.set_ylabel('log(following+1)')\n",
    "\n",
    "#Compare tweet counts\n",
    "sns.violinplot(ax= ax3, y=np.log10(fNN_train[\"tweet_count\"]+1), x=fNN_train[\"veracity\"])\n",
    "ax3.set_title('Tweet count')\n",
    "ax3.set_ylabel('log(tweet_count+1)')\n",
    "\n",
    "#Compare number hashtags\n",
    "sns.violinplot(ax=ax4, y=np.log10(fNN_train[\"num_hashtags\"]+1), x=fNN_train[\"veracity\"])\n",
    "ax4.set_title('Number of hashtags by veracity')\n",
    "ax4.set_ylabel('log(num_hashtags+1)')\n",
    "\n",
    "#Compare number mentions\n",
    "sns.violinplot(ax=ax5, y=np.log10(fNN_train[\"num_mentions\"]+1), x=fNN_train[\"veracity\"])\n",
    "ax5.set_title('Number of mentions by veracity')\n",
    "ax5.set_ylabel('log(num_mentions+1)')\n",
    "\n",
    "#Compare number urls\n",
    "sns.violinplot(ax=ax6, y=np.log10(fNN_train[\"num_urls\"]+1), x=fNN_train[\"veracity\"])\n",
    "ax6.set_title('Number of URLs by veracity')\n",
    "ax6.set_ylabel('log(num_urls+1)')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save vad values as series\n",
    "valence = fNN_train['valence']\n",
    "arousal = fNN_train['arousal']\n",
    "dominance = fNN_train['dominance']\n",
    "fake = fNN_train['veracity'] == 'Fake'\n",
    "\n",
    "fake_val = valence[fake]\n",
    "real_val = valence[~fake]\n",
    "fake_aro = arousal[fake]\n",
    "real_aro = arousal[~fake]\n",
    "fake_dom = dominance[fake]\n",
    "real_dom = dominance[~fake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Welch’s t-tests VAD \n",
    "print(\"Valence:\", ttest_ind(fake_val, real_val, equal_var=False))\n",
    "print(\"Arousal:\", ttest_ind(fake_aro, real_aro, equal_var=False))\n",
    "print(\"Dominance:\", ttest_ind(fake_dom, real_dom, equal_var=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a30e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Cohen's d for independent samples\n",
    "def cohend(d1, d2):\n",
    "    # Calculate the size of samples\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    # Calculate the variance of the samples\n",
    "    s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    "    # Calculate the pooled standard deviation\n",
    "    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    # Calculate the means of the samples\n",
    "    u1, u2 = np.mean(d1), np.mean(d2)\n",
    "    # Calculate effect size\n",
    "    return (u1 - u2) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cohen's d VAD \n",
    "print(\"Cohen's d for valence: \" + str(cohend(fake_val, real_val)))\n",
    "print(\"Cohen's d for arousal: \" + str(cohend(fake_aro, real_aro)))\n",
    "print(\"Cohen's d for dominance: \" + str(cohend(fake_dom, real_dom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin plots for VAD - log(n+1) transformed for clearer visualisation\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(6, 12))\n",
    "\n",
    "#Compare valence\n",
    "sns.violinplot(ax=ax1, y=fNN_train[\"valence\"], x=fNN_train[\"veracity\"])\n",
    "ax1.set_title('Valence')\n",
    "\n",
    "#Compare arousal\n",
    "sns.violinplot(ax=ax2, y=fNN_train[\"arousal\"], x=fNN_train[\"veracity\"])\n",
    "ax2.set_title('Arousal')\n",
    "\n",
    "#Compare dominance\n",
    "sns.violinplot(ax=ax3, y=fNN_train[\"dominance\"], x=fNN_train[\"veracity\"])\n",
    "ax3.set_title('Dominance')\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.set(xlabel='Veracity', ylabel='Score')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9adc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save emotion values as series\n",
    "anger = fNN_train['anger']\n",
    "anticipation = fNN_train['anticipation']\n",
    "disgust = fNN_train['disgust']\n",
    "fear = fNN_train['fear']\n",
    "joy = fNN_train['joy']\n",
    "sadness = fNN_train['sadness']\n",
    "surprise = fNN_train['surprise']\n",
    "trust = fNN_train['trust']\n",
    "\n",
    "fake_anger = anger[fake]\n",
    "real_anger = anger[~fake]\n",
    "fake_antic = anticipation[fake]\n",
    "real_antic = anticipation[~fake]\n",
    "fake_disgust = disgust[fake]\n",
    "real_disgust = disgust[~fake]\n",
    "fake_fear = fear[fake]\n",
    "real_fear = fear[~fake]\n",
    "fake_joy = joy[fake]\n",
    "real_joy = joy[~fake]\n",
    "fake_sadness = sadness[fake]\n",
    "real_sadness = sadness[~fake]\n",
    "fake_surprise = surprise[fake]\n",
    "real_surprise = surprise[~fake]\n",
    "fake_trust = trust[fake]\n",
    "real_trust = trust[~fake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Welch’s t-tests for discrete emotion metrics\n",
    "print(\"Anger:\", ttest_ind(fake_anger, real_anger, equal_var=False))\n",
    "print(\"Anticipation:\", ttest_ind(fake_antic, real_antic, equal_var=False))\n",
    "print(\"Disgust:\", ttest_ind(fake_disgust, real_disgust, equal_var=False))\n",
    "print(\"Fear:\", ttest_ind(fake_fear, real_fear, equal_var=False))\n",
    "print(\"Joy:\", ttest_ind(fake_joy, real_joy, equal_var=False))\n",
    "print(\"Sadness:\", ttest_ind(fake_sadness, real_sadness, equal_var=False))\n",
    "print(\"Suprise:\", ttest_ind(fake_surprise, real_surprise, equal_var=False))\n",
    "print(\"Trust:\", ttest_ind(fake_trust, real_trust, equal_var=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab658c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cohen's d for discrete emotion metrics\n",
    "print(\"Cohen's d for anger: \" + str(cohend(fake_anger, real_anger)))\n",
    "print(\"Cohen's d for anticipation: \" + str(cohend(fake_antic, real_antic)))\n",
    "print(\"Cohen's d for disgust: \" + str(cohend(fake_disgust, real_disgust)))\n",
    "print(\"Cohen's d for fear: \" + str(cohend(fake_fear, real_fear)))\n",
    "print(\"Cohen's d for joy: \" + str(cohend(fake_joy, real_joy)))\n",
    "print(\"Cohen's d for sadness: \" + str(cohend(fake_sadness, real_sadness)))\n",
    "print(\"Cohen's d for surprise: \" + str(cohend(fake_surprise, real_surprise)))\n",
    "print(\"Cohen's d for trust: \" + str(cohend(fake_trust, real_trust)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get binary present/not present for discrete emotion metrics\n",
    "fNN_train['anger_zero'] = np.where(fNN_train['anger']== 0, True, False)\n",
    "fNN_train['antic_zero'] = np.where(fNN_train['anticipation']== 0, True, False)\n",
    "fNN_train['disgust_zero'] = np.where(fNN_train['disgust']== 0, True, False)\n",
    "fNN_train['fear_zero'] = np.where(fNN_train['fear']== 0, True, False)\n",
    "fNN_train['joy_zero'] = np.where(fNN_train['joy']== 0, True, False)\n",
    "fNN_train['sadness_zero'] = np.where(fNN_train['sadness']== 0, True, False)\n",
    "fNN_train['surprise_zero'] = np.where(fNN_train['surprise']== 0, True, False)\n",
    "fNN_train['trust_zero'] = np.where(fNN_train['trust']== 0, True, False)\n",
    "\n",
    "old_cols = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', \n",
    "            'trust']\n",
    "new_cols = ['anger_positive', 'antic_positive', 'disgust_positive', 'fear_positive', \n",
    "            'joy_positive', 'sadness_positive', 'surprise_positive', 'trust_positive']\n",
    "\n",
    "fNN_train[new_cols] = fNN_train[old_cols]\n",
    "fNN_train[new_cols] = fNN_train[new_cols].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots for discrete emotions - log(n+1) transformed for clearer visualisation\n",
    "#Fig setup\n",
    "fig,((ax1, ax2),(ax3, ax4),(ax5, ax6),(ax7, ax8)) = plt.subplots(4, 2, figsize=(12, 12))\n",
    "\n",
    "#Compare anger\n",
    "sns.violinplot(ax=ax1, y=fNN_train[\"anger_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax1.set_title('Anger')\n",
    "\n",
    "#Compare anticipation\n",
    "sns.violinplot(ax=ax2, y=fNN_train[\"antic_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax2.set_title('Anticipation')\n",
    "\n",
    "#Compare disgust\n",
    "sns.violinplot(ax=ax3, y=fNN_train[\"disgust_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax3.set_title('Disgust')\n",
    "\n",
    "#Compare fear\n",
    "sns.violinplot(ax=ax4, y=fNN_train[\"fear_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax4.set_title('Fear')\n",
    "\n",
    "#Compare joy\n",
    "sns.violinplot(ax=ax5, y=fNN_train[\"joy_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax5.set_title('Joy')\n",
    "\n",
    "#Compare disgust\n",
    "sns.violinplot(ax=ax6, y=fNN_train[\"sadness_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax6.set_title('Sadness')\n",
    "\n",
    "#Compare disgust\n",
    "sns.violinplot(ax=ax7, y=fNN_train[\"surprise_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax7.set_title('Surprise')\n",
    "\n",
    "#Compare disgust\n",
    "sns.violinplot(ax=ax8, y=fNN_train[\"trust_positive\"], x=fNN_train[\"veracity\"])\n",
    "ax8.set_title('Trust')\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.set(xlabel='Veracity', ylabel='Emotion Score')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare presence of any level of emotion (binary present or not)\n",
    "len_fake = (fNN_train['veracity']=='Fake').sum()\n",
    "len_real = (fNN_train['veracity']=='Real').sum()\n",
    "\n",
    "emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', \n",
    "            'trust']\n",
    "zeroes = ['anger_zero', 'antic_zero', 'disgust_zero', 'fear_zero', 'joy_zero', \n",
    "          'sadness_zero', 'surprise_zero', 'trust_zero']\n",
    "percentage_fake = []\n",
    "percentage_real = []\n",
    "\n",
    "for i in zeroes:\n",
    "    count = fNN_train.groupby([i,'veracity']).count()\n",
    "    count[count.columns[0]]\n",
    "    percent_fake = count[count.columns[0]][0][0]/len_fake*100\n",
    "    percentage_fake.append(percent_fake)\n",
    "    percent_real = count[count.columns[0]][0][1]/len_real*100\n",
    "    percentage_real.append(percent_real)\n",
    "\n",
    "#Percentage of tweets containing each discrete emotion\n",
    "percent_df = pd.DataFrame(list(zip(emotions, percentage_fake, percentage_real)), \n",
    "                          columns =['emotion', 'fake', 'real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot discrete emotion tweet percentages\n",
    "percent_df.plot(x=\"emotion\", y=[\"fake\", \"real\"], kind=\"bar\",figsize=(9,8))\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Emotion')\n",
    "plt.legend(title=\"Veracity\", fontsize='small', fancybox=True)\n",
    "plt.title('Percentage of tweets that contain each emotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a459d",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "- Check correlations\n",
    "- Develop logistic regression models with and without emotion metric features\n",
    "- Likelihood-ratio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2dd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change veracity labels to binary 0/1\n",
    "y_train = y_train['veracity'].map({'Real': 0, 'Fake': 1})\n",
    "y_val = y_val['veracity'].map({'Real': 0, 'Fake': 1})\n",
    "y_test = y_test['veracity'].map({'Real': 0, 'Fake': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add constant to provide intercept\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_val = sm.add_constant(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b195a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with all variables\n",
    "logit = sm.Logit(endog = y_train, exog = X_train)\n",
    "full_model = logit.fit()\n",
    "full_ll = full_model.llf\n",
    "print(full_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e2ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partial logistic regression model without emotions\n",
    "partial_cols = ['followers', 'following', 'tweet_count', 'num_hashtags', 'num_mentions', \n",
    "                'num_urls', 'tweet_length']\n",
    "\n",
    "X_train_partial = X_train[partial_cols]\n",
    "\n",
    "logit_p = sm.Logit(endog = y_train, exog = X_train_partial)\n",
    "partial_model = logit_p.fit()\n",
    "partial_ll = partial_model.llf\n",
    "print(partial_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db810eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log-likelihood test\n",
    "print(\"Partial LL:\", partial_ll)\n",
    "print(\"Full LL:\", full_ll)\n",
    "#calculate likelihood ratio test statistic\n",
    "LR_statistic = -2*(partial_ll - full_ll)\n",
    "\n",
    "#Calculate p-value of test statistic using 11 degrees of freedom \n",
    "# 11 is difference between models' degrees of freedom\n",
    "p_val = scipy.stats.chi2.sf(LR_statistic, 11)\n",
    "\n",
    "print('LR test, p value: {:.2f}, {:.4f}'.format(LR_statistic, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3dde42",
   "metadata": {},
   "source": [
    "##### Statistically significant (p<0.0001) so reject null - model with emotion metric features fits data better than model without "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a4844",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Standardize feature values\n",
    "- For each of logistic regression with stochastic gradient descent (SGD), decision tree, random forest and XGBoost:\n",
    "    - Grid search optimisation\n",
    "    - Train model with optimal hyperparameters\n",
    "    - Test on testing dataset to get confusion matrix\n",
    "    - Bootstrap testing dataset to get confidence intervals for evaluation metrics\n",
    "- ROC and precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164be6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24347a79",
   "metadata": {},
   "source": [
    "## Logistic Regression with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce648fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search for optimal parameters \n",
    "params = {\n",
    "    \"alpha\" : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"penalty\" : [\"l2\", \"l1\", \"elasticnet\"],\n",
    "}\n",
    "\n",
    "sgd = SGDClassifier(class_weight=\"balanced\", loss=\"log\")\n",
    "GS_sgd = GridSearchCV(sgd, param_grid=params, cv=3, scoring='average_precision')\n",
    "\n",
    "GS_sgd.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87730bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(GS_sgd.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29165276",
   "metadata": {},
   "source": [
    "##### Optimal parameters: {'alpha': 0.01, 'penalty': 'l2'}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ced98",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = SGDClassifier(class_weight=\"balanced\", \n",
    "                             loss=\"log\", \n",
    "                             alpha=0.01, \n",
    "                             penalty=\"l2\")\n",
    "\n",
    "logreg_model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict with optimal hyperparameters\n",
    "logreg_y_pred = logreg_model.predict(X_test_std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "logreg_cnf_matrix = confusion_matrix(y_test, logreg_y_pred)\n",
    "#heatmap\n",
    "ax1 = sns.heatmap(pd.DataFrame(logreg_cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "ax1.set_yticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_xticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_title(\"LogReg Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172aec24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, logreg_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print mean and 95% confidence intervals for bootstrapped test data\n",
    "def summary_stats(accuracy, precision, recall, f1, pr_auc):\n",
    "    \n",
    "    #Accuracy\n",
    "    mean_acc = np.mean(accuracy)\n",
    "    lower_acc = np.percentile(accuracy, 2.5)\n",
    "    upper_acc = np.percentile(accuracy, 97.5)\n",
    "\n",
    "    #Precision\n",
    "    mean_prec = np.mean(precision)\n",
    "    lower_prec = np.percentile(precision, 2.5)\n",
    "    upper_prec = np.percentile(precision, 97.5)\n",
    "\n",
    "    #Recall\n",
    "    mean_rec = np.mean(recall)\n",
    "    lower_rec = np.percentile(recall, 2.5)\n",
    "    upper_rec = np.percentile(recall, 97.5)\n",
    "\n",
    "    #F1\n",
    "    mean_f1 = np.mean(f1)\n",
    "    lower_f1= np.percentile(f1, 2.5)\n",
    "    upper_f1 = np.percentile(f1, 97.5)\n",
    "\n",
    "    #PR-AUC\n",
    "    mean_auc = np.mean(pr_auc)\n",
    "    lower_auc = np.percentile(pr_auc, 2.5)\n",
    "    upper_auc = np.percentile(pr_auc, 97.5)\n",
    "     \n",
    "\n",
    "    print(\"Mean Accuracy: %.3f\" % (mean_acc), \"-- CI: %.3f\" % (lower_acc), \n",
    "          \"- %.3f\" % (upper_acc))\n",
    "    print(\"Mean Precision: %.3f\" % (mean_prec), \"-- CI: %.3f\" % (lower_prec), \n",
    "          \"- %.3f\" % (upper_prec))\n",
    "    print(\"Mean Recall: %.3f\" % (mean_rec), \"-- CI: %.3f\" % (lower_rec), \n",
    "          \"- %.3f\" % (upper_rec))\n",
    "    print(\"Mean F1: %.3f\" % (mean_f1), \"-- CI: %.3f\" % (lower_f1), \n",
    "          \"- %.3f\" % (upper_f1))\n",
    "    print(\"Mean PR-AUC: %.3f\" % (mean_auc), \"-- CI: %.3f\" % (lower_auc), \n",
    "          \"- %.3f\" % (upper_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap predictions to get confidence intervals\n",
    "rng = np.random.RandomState(seed=42)\n",
    "index = np.arange(y_test.shape[0])\n",
    "logreg_boot_accs = []\n",
    "logreg_boot_precs = []\n",
    "logreg_boot_recs = []\n",
    "logreg_boot_f1s = []\n",
    "logreg_boot_pr_aucs = []\n",
    "\n",
    "for i in range(200):\n",
    "    #Bootstrap sample\n",
    "    pred_index = rng.choice(index, size=index.shape[0], replace=True)\n",
    "    #Accuracy\n",
    "    logreg_boot_acc = metrics.accuracy_score(y_test[pred_index], \n",
    "                                             logreg_y_pred[pred_index])\n",
    "    logreg_boot_accs.append(logreg_boot_acc)\n",
    "    #Precision\n",
    "    logreg_boot_prec = metrics.precision_score(y_test[pred_index], \n",
    "                                               logreg_y_pred[pred_index])\n",
    "    logreg_boot_precs.append(logreg_boot_prec)\n",
    "    #Recall\n",
    "    logreg_boot_rec = metrics.recall_score(y_test[pred_index], \n",
    "                                           logreg_y_pred[pred_index])\n",
    "    logreg_boot_recs.append(logreg_boot_rec)\n",
    "    #F1\n",
    "    logreg_boot_f1 = metrics.f1_score(y_test[pred_index], \n",
    "                                      logreg_y_pred[pred_index])\n",
    "    logreg_boot_f1s.append(logreg_boot_f1)    \n",
    "    #AUC-PR\n",
    "    logreg_precision, logreg_recall, _ = precision_recall_curve(y_test[pred_index], \n",
    "                                                                logreg_prob[pred_index])\n",
    "    logreg_pr_auc = auc(logreg_recall, logreg_precision)\n",
    "    logreg_boot_pr_aucs.append(logreg_pr_auc)\n",
    "    \n",
    "summary_stats(logreg_boot_accs, logreg_boot_precs, logreg_boot_recs, \n",
    "              logreg_boot_f1s, logreg_boot_pr_aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d3af9",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23774852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search for optimal parameters \n",
    "params = {\n",
    "    \"criterion\" : [\"gini\", \"entropy\" ],\n",
    "    \"max_depth\" : [18, 19, 20, 21, 22],\n",
    "    \"min_samples_split\": [8, 9, 10, 11, 12, 13],\n",
    "    \"min_samples_leaf\": [4, 5, 6, 7, 8]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "GS_dt = GridSearchCV(dt, param_grid=params, scoring='average_precision', cv=3)\n",
    "\n",
    "GS_dt.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b948a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(GS_dt.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594fc72",
   "metadata": {},
   "source": [
    "##### Optimal parameters: {'criterion': 'gini', 'max_depth': 22, 'min_samples_leaf': 8, 'min_samples_split': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06700e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion='gini',\n",
    "                                  max_depth=22, \n",
    "                                  min_samples_leaf=8,\n",
    "                                  min_samples_split=9)\n",
    "\n",
    "dt_model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35713709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "dt_y_pred = dt_model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475abde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "dt_cnf_matrix = confusion_matrix(y_test, dt_y_pred)\n",
    "#heatmap\n",
    "ax1 = sns.heatmap(pd.DataFrame(dt_cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "ax1.set_yticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_xticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_title(\"Decision Tree Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f49ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, dt_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap predictions to get confidence intervals\n",
    "rng = np.random.RandomState(seed=42)\n",
    "index = np.arange(y_test.shape[0])\n",
    "dt_boot_accs = []\n",
    "dt_boot_precs = []\n",
    "dt_boot_recs = []\n",
    "dt_boot_f1s = []\n",
    "dt_boot_pr_aucs = []\n",
    "\n",
    "for i in range(200):\n",
    "    #Bootstrap sample\n",
    "    pred_index = rng.choice(index, size=index.shape[0], replace=True)\n",
    "    #Accuracy\n",
    "    dt_boot_acc = metrics.accuracy_score(y_test[pred_index], dt_y_pred[pred_index])\n",
    "    dt_boot_accs.append(dt_boot_acc)\n",
    "    #Precision\n",
    "    dt_boot_prec = metrics.precision_score(y_test[pred_index], dt_y_pred[pred_index])\n",
    "    dt_boot_precs.append(dt_boot_prec)\n",
    "    #Recall\n",
    "    dt_boot_rec = metrics.recall_score(y_test[pred_index], dt_y_pred[pred_index])\n",
    "    dt_boot_recs.append(dt_boot_rec)\n",
    "    #F1\n",
    "    dt_boot_f1 = metrics.f1_score(y_test[pred_index], dt_y_pred[pred_index])\n",
    "    dt_boot_f1s.append(dt_boot_f1)    \n",
    "    #AUC-PR\n",
    "    dt_precision, dt_recall, _ = precision_recall_curve(y_test[pred_index], \n",
    "                                                        dt_prob[pred_index])\n",
    "    dt_pr_auc = auc(dt_recall, dt_precision)\n",
    "    dt_boot_pr_aucs.append(dt_pr_auc)\n",
    "    \n",
    "summary_stats(dt_boot_accs, dt_boot_precs, dt_boot_recs, dt_boot_f1s, dt_boot_pr_aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e415ca",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5089eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search for optimal parameters \n",
    "params = { \n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'max_depth' : [3, 5, 7, None],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "GS_rf = GridSearchCV(rf, param_grid=params, scoring='average_precision', cv=3)\n",
    "\n",
    "GS_rf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520902b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(GS_rf.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547c920",
   "metadata": {},
   "source": [
    "##### Optimal parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(criterion='entropy',\n",
    "                            max_depth=None,\n",
    "                            max_features='sqrt',\n",
    "                            n_estimators=500)\n",
    "\n",
    "rf_model.fit(X_train_std, y_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "rf_y_pred = rf_model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c503f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "rf_cnf_matrix = confusion_matrix(y_test, rf_y_pred)\n",
    "#heatmap\n",
    "ax1 = sns.heatmap(pd.DataFrame(rf_cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "ax1.set_yticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_xticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c11e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap predictions to get confidence intervals\n",
    "rng = np.random.RandomState(seed=42)\n",
    "index = np.arange(y_test.shape[0])\n",
    "rf_boot_accs = []\n",
    "rf_boot_precs = []\n",
    "rf_boot_recs = []\n",
    "rf_boot_f1s = []\n",
    "rf_boot_pr_aucs = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    #Bootstrap sample\n",
    "    pred_index = rng.choice(index, size=index.shape[0], replace=True)\n",
    "    #Accuracy\n",
    "    rf_boot_acc = metrics.accuracy_score(y_test[pred_index], rf_y_pred[pred_index])\n",
    "    rf_boot_accs.append(rf_boot_acc)\n",
    "    #Precision\n",
    "    rf_boot_prec = metrics.precision_score(y_test[pred_index], rf_y_pred[pred_index])\n",
    "    rf_boot_precs.append(rf_boot_prec)\n",
    "    #Recall\n",
    "    rf_boot_rec = metrics.recall_score(y_test[pred_index], rf_y_pred[pred_index])\n",
    "    rf_boot_recs.append(rf_boot_rec)\n",
    "    #F1\n",
    "    rf_boot_f1 = metrics.f1_score(y_test[pred_index], rf_y_pred[pred_index])\n",
    "    rf_boot_f1s.append(rf_boot_f1)    \n",
    "    #AUC-PR\n",
    "    rf_precision, rf_recall, _ = precision_recall_curve(y_test[pred_index], \n",
    "                                                        rf_prob[pred_index])\n",
    "    rf_pr_auc = auc(rf_recall, rf_precision)\n",
    "    rf_boot_pr_aucs.append(rf_pr_auc)\n",
    "    \n",
    "summary_stats(rf_boot_accs, rf_boot_precs, rf_boot_recs, rf_boot_f1s, rf_boot_pr_aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b59316",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [15, 16, 17, 18, 19],\n",
    "    'learning_rate': [0.1, 0.01, 0.05],\n",
    "    'gamma': [0, 0.25, 1.0],\n",
    "    'reg_lambda': [0, 1.0, 10.0],\n",
    "    'scale_pos_weight': [1, 3, 5] \n",
    "}\n",
    "\n",
    "GS_xgb = GridSearchCV(estimator=xgb.XGBClassifier(objective='binary:logistic', \n",
    "                                                  use_label_encoder=False),\n",
    "                      param_grid=params,\n",
    "                      scoring='average_precision', \n",
    "                      n_jobs = -1,\n",
    "                      cv = 3)\n",
    "\n",
    "GS_xgb.fit(X_train_std, y_train, \n",
    "           early_stopping_rounds=10,                \n",
    "           eval_metric='aucpr',\n",
    "           eval_set=[(X_val_std, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc07ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f12f0a",
   "metadata": {},
   "source": [
    "##### Optimal parameters: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 17, 'reg_lambda': 0, 'scale_pos_weight': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', \n",
    "                              use_label_encoder=False, \n",
    "                              gamma=0, \n",
    "                              learning_rate=0.1, \n",
    "                              max_depth=17, \n",
    "                              reg_lambda=0, \n",
    "                              scale_pos_weight=1)\n",
    "\n",
    "xgb_model.fit(X_train_std, y_train, \n",
    "              early_stopping_rounds=10,                \n",
    "              eval_metric='aucpr',\n",
    "              eval_set=[(X_val_std, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423dbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "xgb_y_pred = xgb_model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9373d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "xgb_cnf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
    "#heatmap\n",
    "ax1 = sns.heatmap(pd.DataFrame(xgb_cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "ax1.set_yticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_xticklabels([\"Real\", \"Fake\"])\n",
    "ax1.set_title(\"XGBoost Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20469c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, xgb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap predictions to get confidence intervals\n",
    "rng = np.random.RandomState(seed=42)\n",
    "index = np.arange(y_test.shape[0])\n",
    "xgb_boot_accs = []\n",
    "xgb_boot_precs = []\n",
    "xgb_boot_recs = []\n",
    "xgb_boot_f1s = []\n",
    "xgb_boot_pr_aucs = []\n",
    "\n",
    "for i in range(200):\n",
    "    #Bootstrap sample\n",
    "    pred_index = rng.choice(index, size=index.shape[0], replace=True)\n",
    "    #Accuracy\n",
    "    xgb_boot_acc = metrics.accuracy_score(y_test[pred_index], xgb_y_pred[pred_index])\n",
    "    xgb_boot_accs.append(xgb_boot_acc)\n",
    "    #Precision\n",
    "    xgb_boot_prec = metrics.precision_score(y_test[pred_index], xgb_y_pred[pred_index])\n",
    "    xgb_boot_precs.append(xgb_boot_prec)\n",
    "    #Recall\n",
    "    xgb_boot_rec = metrics.recall_score(y_test[pred_index], xgb_y_pred[pred_index])\n",
    "    xgb_boot_recs.append(xgb_boot_rec)\n",
    "    #F1\n",
    "    xgb_boot_f1 = metrics.f1_score(y_test[pred_index], xgb_y_pred[pred_index])\n",
    "    xgb_boot_f1s.append(xgb_boot_f1)    \n",
    "    #AUC-PR\n",
    "    xgb_precision, xgb_recall, _ = precision_recall_curve(y_test[pred_index], \n",
    "                                                          xgb_prob[pred_index])\n",
    "    xgb_pr_auc = auc(xgb_recall, xgb_precision)\n",
    "    xgb_boot_pr_aucs.append(xgb_pr_auc)\n",
    "    \n",
    "summary_stats(xgb_boot_accs, xgb_boot_precs, xgb_boot_recs, \n",
    "              xgb_boot_f1s, xgb_boot_pr_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curves\n",
    "#Predicting only majority class\n",
    "no_model_pred = [0 for i in range(len(y_test))]\n",
    "#ROC curve for no model\n",
    "no_model_fpr, no_model_tpr, _ = roc_curve(y_test, no_model_pred)\n",
    "#AUC for no model\n",
    "no_model_auc = roc_auc_score(y_test, no_model_pred)\n",
    "\n",
    "#Calculate ROC curves for models\n",
    "#Predict probabilities\n",
    "sgd_prob = sgd_model.predict_proba(X_test_std)[:, 1]\n",
    "dt_prob = dt_model.predict_proba(X_test_std)[:, 1]\n",
    "rf_prob = rf_model.predict_proba(X_test_std)[:, 1]\n",
    "xgb_prob = xgb_model.predict_proba(X_test_std)[:, 1]\n",
    "\n",
    "#Calculate ROC AUC\n",
    "sgd_auc = roc_auc_score(y_test, sgd_prob)\n",
    "dt_auc = roc_auc_score(y_test, dt_prob)\n",
    "rf_auc = roc_auc_score(y_test, rf_prob)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_prob)\n",
    "\n",
    "#Calculate true positive and false positive rate\n",
    "sgd_fpr, sgd_tpr, _ = roc_curve(y_test, sgd_prob)\n",
    "dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_prob)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_prob)\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_prob)\n",
    "\n",
    "#calculate precision and recall\n",
    "no_model_precision, no_model_recall, _ = precision_recall_curve(y_test, no_model_pred)\n",
    "sgd_precision, sgd_recall, _ = precision_recall_curve(y_test, sgd_prob)\n",
    "dt_precision, dt_recall, _ = precision_recall_curve(y_test, dt_prob)\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, rf_prob)\n",
    "xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, xgb_prob)\n",
    "\n",
    "#Calculate AUC-PR\n",
    "no_model_pr_auc = auc(no_model_recall, no_model_precision)\n",
    "sgd_pr_auc = auc(sgd_recall, sgd_precision)\n",
    "dt_pr_auc = auc(dt_recall, dt_precision)\n",
    "rf_pr_auc = auc(rf_recall, rf_precision)\n",
    "xgb_pr_auc = auc(xgb_recall, xgb_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e307997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve labels\n",
    "no_model_label = 'No model: AUC=%.2f' % (no_model_auc)\n",
    "sgd_label = 'Logistic SGD: AUC=%.2f' % (sgd_auc)\n",
    "dt_label = 'Decision Tree: AUC=%.2f' % (dt_auc)\n",
    "rf_label = 'Random Forest: AUC=%.2f' % (rf_auc)\n",
    "xgb_label = 'XGBoost: AUC=%.2f' % (xgb_auc)\n",
    "\n",
    "#Plot ROC curves for models\n",
    "plt.plot(no_model_fpr, no_model_tpr, linestyle='--', label=no_model_label)\n",
    "plt.plot(sgd_fpr, sgd_tpr, marker='.', label= sgd_label)\n",
    "plt.plot(dt_fpr, dt_tpr, marker='.', label= dt_label)\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label= rf_label)\n",
    "plt.plot(xgb_fpr, xgb_tpr, marker='.', label= xgb_label)\n",
    "\n",
    "#Axes labels and legend\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ef86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PR curve labels\n",
    "no_model_label = 'No model: AUC-PR=%.2f' % (no_model_pr_auc)\n",
    "sgd_label = 'Logistic SGD: AUC-PR=%.2f' % (sgd_pr_auc)\n",
    "dt_label = 'Decision Tree: AUC-PR=%.2f' % (dt_pr_auc)\n",
    "rf_label = 'Random Forest: AUC-PR=%.2f' % (rf_pr_auc)\n",
    "xgb_label = 'XGBoost: AUC-PR=%.2f' % (xgb_pr_auc)\n",
    "\n",
    "\n",
    "#Plot PR curve for models\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sgd_recall, sgd_precision, color='orange', label=sgd_label)\n",
    "ax.plot(dt_recall, dt_precision, color='green', label=dt_label)\n",
    "ax.plot(rf_recall, rf_precision, color='red', label=rf_label)\n",
    "ax.plot(xgb_recall, xgb_precision, color='purple', label=xgb_label)\n",
    "\n",
    "no_model = len(y_test[y_test==1]) / len(y_test)\n",
    "ax.plot([0, 1], [no_model, no_model], linestyle='--', label=no_model_label)\n",
    "\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "\n",
    "# Shrink current axis by 20%\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# plt.legend()\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6b182",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "- SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std = pd.DataFrame(X_test_std, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b478bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Absolute means\n",
    "shap.plots.bar(shap_values.abs.mean(0), max_display=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate standard errors for absolute means\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=X_test_std.columns)\n",
    "\n",
    "for column in shap_df.columns:\n",
    "    print(column, \"SHAP SE: %.4f\" % (sem(shap_df[column])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.plots.beeswarm(shap_values, max_display=19, show=False)\n",
    "plt.gcf().axes[-1].set_aspect('auto')\n",
    "plt.tight_layout()\n",
    "# As mentioned, smaller \"box_aspect\" value to make colorbar thicker\n",
    "plt.gcf().axes[-1].set_box_aspect(100) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
